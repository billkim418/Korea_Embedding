# 한국어 임베딩 2장

### 2.1 자연어 계산과 이해

임베딩 : 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 바꾼 결과

idea : 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는 것

| 구분 | 백오브워즈 가정 | 언어 모델 | 분포 가정 |
| --- | --- | --- | --- |
| 내용 | 어떤 단어가 (많이) 쓰였는가 | 단어가 어떤 순서로 쓰였는가 | 어떤 단어가 같이 쓰였는가 |
| 대표 통계량 | TF-IDF | - | PMI |
| 대표 모델 | Deep Averaging Network | ELMo, GPT | Word2Vec |

### 2.2 어떤 단어가 많이 쓰였는가

백오브 워즈(bag of words) : 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법

ex) 단어 - 문서 행렬(Term -Document matrix)

| 구분 | 메밀꽃 필 무렵 | 운수 좋은 날 | 사랑 손님과 어머니 | 삼포 가는 길 |
| --- | --- | --- | --- | --- |
| 기차 | 0 | 2 | 10 | 7 |
| 막걸리 | 0 | 1 | 0 | 0 |
| 선술집 | 0 | 1 | 0 | 0 |

정보 검색(Information Rretrival) 분야에서 많이 사용되고 있음(코사인 유사도 이용)

TF-IDF(Term Frequency-Inverse Document Frequency)

![Untitled](%E1%84%92%E1%85%A1%E1%86%AB%E1%84%80%E1%85%AE%E1%86%A8%E1%84%8B%E1%85%A5%20%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC%202%E1%84%8C%E1%85%A1%E1%86%BC%202653db3fdde54df0bc92d5a1d7ba3d26/Untitled.png)

Idea : 단어 사용 빈도는 저자가 상정한 주제와 관련을 맺고 있을 것

TF-IDF(행렬)

| 구분 | 메밀꽃 필 무렵 | 운수 좋은 날 | 사랑 손님과 어머니 | 삼포 가는 길 |
| --- | --- | --- | --- | --- |
| 어머니 | 0.066 | 0.0 | 0.595 | 0.0 |
| 막걸리 | 0.2622 | 0.098 | 0.145 | 0.0848 |

Deep Averaging Network

![Untitled](%E1%84%92%E1%85%A1%E1%86%AB%E1%84%80%E1%85%AE%E1%86%A8%E1%84%8B%E1%85%A5%20%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC%202%E1%84%8C%E1%85%A1%E1%86%BC%202653db3fdde54df0bc92d5a1d7ba3d26/Untitled%201.png)

특징 : 백오브워즈 가정의 뉴럴 네트워크 

문장의 임베딩은 중복집합에 속한 단어의 임베딩 결과에 평균을 취해 만든다. 그렇기 때문에 단어의 순서를 고려하지 않는다.

특히 해당 문서가 어떤 범주인지 분류하는 작업을 주로 수행하고, 성능이 좋아 현업에서 자주 사용한다.

## 2.3 단어가 어떤 순서로 쓰였는가

통계 기반 언어 모델

언어 모델(통계 기반) : 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습한다.

단점 : 말뭉치에 등장하지 않는 표현은 확률이 0으로 계산된다.

n-gram 모델 :